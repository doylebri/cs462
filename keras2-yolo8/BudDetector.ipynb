{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "055aed2a-c3e1-46c7-b593-5f9de3f24e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTALL_PACKAGES = False\n",
    "\n",
    "# we're going to use tensorflow 2.14.0 and keras 2.14.0 because that's probably what the tutorial used\n",
    "# https://developer.apple.com/metal/tensorflow-plugin/\n",
    "# KerasCV installation: https://keras.io/keras_cv/#keras-2-installation\n",
    "if INSTALL_PACKAGES:\n",
    "    !pip install tensorflow==2.14.0 tensorflow-metal keras-cv opencv-python pycocotools matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ce75de-68a4-40af-97bf-2a0b86a553ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import keras_cv\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv import visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab95cad-ad11-4756-9496-48be022e9a70",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98180f0-a550-4400-9bcc-528a32b76a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# SCALE_MAX = 1.3\n",
    "# SCALE_MIN = 0.75\n",
    "SCALE_MAX = 1.0\n",
    "SCALE_MIN = 1.0\n",
    "SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50\n",
    "GLOBAL_CLIPNORM = 10.0\n",
    "USE_RAGGED_TENSORS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4faf5b94-ac14-4051-95bc-780cb92ecd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Bud', 1: 'Stem'}\n",
      "dict_values(['Bud', 'Stem'])\n",
      "['data/annotations/02d71183-IMG_6937.xml', 'data/annotations/081b0c8e-IMG_6939.xml', 'data/annotations/093b9a0e-IMG_6790.xml', 'data/annotations/09997155-IMG_6933.xml', 'data/annotations/12a96634-IMG_6762.xml', 'data/annotations/23be9e87-IMG_6929.xml', 'data/annotations/26598a44-IMG_6921.xml', 'data/annotations/2e492c86-IMG_6440.xml', 'data/annotations/44523aa4-IMG_6931.xml', 'data/annotations/4b03862e-IMG_6923.xml', 'data/annotations/63c48c4f-IMG_6936.xml', 'data/annotations/6cf840cd-IMG_6927.xml', 'data/annotations/775d6964-IMG_6918.xml', 'data/annotations/7c5321ca-IMG_6934.xml', 'data/annotations/88969803-IMG_6920.xml', 'data/annotations/8ae4834c-IMG_6919.xml', 'data/annotations/8c8007be-IMG_6922.xml', 'data/annotations/9758a822-IMG_6924.xml', 'data/annotations/995c3bd7-IMG_6917.xml', 'data/annotations/9979093c-IMG_6935.xml', 'data/annotations/a11d5427-IMG_6930.xml', 'data/annotations/a2409362-IMG_6800.xml', 'data/annotations/b1ca7390-IMG_6916.xml', 'data/annotations/dc05dee6-IMG_6926.xml', 'data/annotations/e7d02260-IMG_6932.xml', 'data/annotations/ef7e6683-IMG_6915.xml', 'data/annotations/f54cb875-IMG_6928.xml', 'data/annotations/f65a8941-IMG_6925.xml']\n",
      "['data/images/02d71183-IMG_6937.jpeg', 'data/images/081b0c8e-IMG_6939.jpeg', 'data/images/093b9a0e-IMG_6790.jpeg', 'data/images/09997155-IMG_6933.jpeg', 'data/images/12a96634-IMG_6762.jpeg', 'data/images/23be9e87-IMG_6929.jpeg', 'data/images/26598a44-IMG_6921.jpeg', 'data/images/2e492c86-IMG_6440.jpeg', 'data/images/44523aa4-IMG_6931.jpeg', 'data/images/4b03862e-IMG_6923.jpeg', 'data/images/63c48c4f-IMG_6936.jpeg', 'data/images/6cf840cd-IMG_6927.jpeg', 'data/images/775d6964-IMG_6918.jpeg', 'data/images/7c5321ca-IMG_6934.jpeg', 'data/images/88969803-IMG_6920.jpeg', 'data/images/8ae4834c-IMG_6919.jpeg', 'data/images/8c8007be-IMG_6922.jpeg', 'data/images/9758a822-IMG_6924.jpeg', 'data/images/995c3bd7-IMG_6917.jpeg', 'data/images/9979093c-IMG_6935.jpeg', 'data/images/a11d5427-IMG_6930.jpeg', 'data/images/a2409362-IMG_6800.jpeg', 'data/images/b1ca7390-IMG_6916.jpeg', 'data/images/dc05dee6-IMG_6926.jpeg', 'data/images/e7d02260-IMG_6932.jpeg', 'data/images/ef7e6683-IMG_6915.jpeg', 'data/images/f54cb875-IMG_6928.jpeg', 'data/images/f65a8941-IMG_6925.jpeg']\n"
     ]
    }
   ],
   "source": [
    "# map class names to unique integer identifier\n",
    "class_ids = [\n",
    "    \"Bud\",\n",
    "    \"Stem\",\n",
    "    \"Leaf\",\n",
    "    \"Leaf-Attachment\",\n",
    "]\n",
    "\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "class_mapping = {0: 'Bud', 1: 'Stem'}\n",
    "class_mapping_values = class_mapping.values()\n",
    "\n",
    "print(class_mapping)\n",
    "print(class_mapping_values)\n",
    "\n",
    "# Path to images and annotations\n",
    "path_annot = \"data/annotations/\"\n",
    "path_images = \"data/images/\"\n",
    "\n",
    "# Get all XML file paths in path_annot and sort them\n",
    "xml_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_annot, file_name)\n",
    "        for file_name in os.listdir(path_annot)\n",
    "        if file_name.endswith(\".xml\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(xml_files)\n",
    "\n",
    "# Get all JPEG image file paths in path_images and sort them\n",
    "jpg_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_images, file_name)\n",
    "        for file_name in os.listdir(path_images)\n",
    "        if file_name.endswith(\".jpeg\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(jpg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32ed5342-a457-4bc9-9997-4aad52b403d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c503ca1e6b24bb49b666a449e259036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02d71183-IMG_6937.jpeg, {'Bud': 6, 'Stem': 2, 'Leaf': 0, 'Leaf-Attachment': 0}\n",
      "081b0c8e-IMG_6939.jpeg, {'Bud': 5, 'Stem': 2, 'Leaf': 0, 'Leaf-Attachment': 0}\n",
      "093b9a0e-IMG_6790.jpeg, {'Bud': 1, 'Stem': 0, 'Leaf': 0, 'Leaf-Attachment': 0}\n",
      "09997155-IMG_6933.jpeg, {'Bud': 2, 'Stem': 2, 'Leaf': 0, 'Leaf-Attachment': 0}\n",
      "12a96634-IMG_6762.jpeg, {'Bud': 4, 'Stem': 8, 'Leaf': 8, 'Leaf-Attachment': 1}\n",
      "23be9e87-IMG_6929.jpeg, {'Bud': 7, 'Stem': 2, 'Leaf': 5, 'Leaf-Attachment': 0}\n",
      "26598a44-IMG_6921.jpeg, {'Bud': 8, 'Stem': 15, 'Leaf': 8, 'Leaf-Attachment': 3}\n",
      "2e492c86-IMG_6440.jpeg, {'Bud': 0, 'Stem': 8, 'Leaf': 0, 'Leaf-Attachment': 0}\n",
      "44523aa4-IMG_6931.jpeg, {'Bud': 2, 'Stem': 2, 'Leaf': 0, 'Leaf-Attachment': 0}\n",
      "4b03862e-IMG_6923.jpeg, {'Bud': 3, 'Stem': 6, 'Leaf': 10, 'Leaf-Attachment': 4}\n",
      "63c48c4f-IMG_6936.jpeg, {'Bud': 3, 'Stem': 1, 'Leaf': 0, 'Leaf-Attachment': 0}\n",
      "6cf840cd-IMG_6927.jpeg, {'Bud': 2, 'Stem': 2, 'Leaf': 1, 'Leaf-Attachment': 0}\n",
      "775d6964-IMG_6918.jpeg, {'Bud': 4, 'Stem': 1, 'Leaf': 6, 'Leaf-Attachment': 1}\n",
      "7c5321ca-IMG_6934.jpeg, {'Bud': 2, 'Stem': 3, 'Leaf': 2, 'Leaf-Attachment': 0}\n",
      "88969803-IMG_6920.jpeg, {'Bud': 5, 'Stem': 2, 'Leaf': 7, 'Leaf-Attachment': 2}\n",
      "8ae4834c-IMG_6919.jpeg, {'Bud': 4, 'Stem': 6, 'Leaf': 3, 'Leaf-Attachment': 4}\n",
      "8c8007be-IMG_6922.jpeg, {'Bud': 7, 'Stem': 6, 'Leaf': 10, 'Leaf-Attachment': 3}\n",
      "9758a822-IMG_6924.jpeg, {'Bud': 4, 'Stem': 7, 'Leaf': 3, 'Leaf-Attachment': 1}\n",
      "995c3bd7-IMG_6917.jpeg, {'Bud': 6, 'Stem': 3, 'Leaf': 0, 'Leaf-Attachment': 2}\n",
      "9979093c-IMG_6935.jpeg, {'Bud': 3, 'Stem': 1, 'Leaf': 0, 'Leaf-Attachment': 0}\n",
      "a11d5427-IMG_6930.jpeg, {'Bud': 5, 'Stem': 0, 'Leaf': 0, 'Leaf-Attachment': 0}\n",
      "a2409362-IMG_6800.jpeg, {'Bud': 2, 'Stem': 2, 'Leaf': 5, 'Leaf-Attachment': 1}\n",
      "b1ca7390-IMG_6916.jpeg, {'Bud': 5, 'Stem': 8, 'Leaf': 4, 'Leaf-Attachment': 1}\n",
      "dc05dee6-IMG_6926.jpeg, {'Bud': 1, 'Stem': 2, 'Leaf': 1, 'Leaf-Attachment': 0}\n",
      "e7d02260-IMG_6932.jpeg, {'Bud': 2, 'Stem': 2, 'Leaf': 0, 'Leaf-Attachment': 0}\n",
      "ef7e6683-IMG_6915.jpeg, {'Bud': 5, 'Stem': 5, 'Leaf': 1, 'Leaf-Attachment': 2}\n",
      "f54cb875-IMG_6928.jpeg, {'Bud': 4, 'Stem': 2, 'Leaf': 4, 'Leaf-Attachment': 0}\n",
      "f65a8941-IMG_6925.jpeg, {'Bud': 1, 'Stem': 2, 'Leaf': 2, 'Leaf-Attachment': 0}\n",
      "25 image paths: ['data/images/02d71183-IMG_6937.jpeg', 'data/images/081b0c8e-IMG_6939.jpeg', 'data/images/09997155-IMG_6933.jpeg', 'data/images/12a96634-IMG_6762.jpeg', 'data/images/23be9e87-IMG_6929.jpeg', 'data/images/26598a44-IMG_6921.jpeg', 'data/images/44523aa4-IMG_6931.jpeg', 'data/images/4b03862e-IMG_6923.jpeg', 'data/images/63c48c4f-IMG_6936.jpeg', 'data/images/6cf840cd-IMG_6927.jpeg', 'data/images/775d6964-IMG_6918.jpeg', 'data/images/7c5321ca-IMG_6934.jpeg', 'data/images/88969803-IMG_6920.jpeg', 'data/images/8ae4834c-IMG_6919.jpeg', 'data/images/8c8007be-IMG_6922.jpeg', 'data/images/9758a822-IMG_6924.jpeg', 'data/images/995c3bd7-IMG_6917.jpeg', 'data/images/9979093c-IMG_6935.jpeg', 'data/images/a2409362-IMG_6800.jpeg', 'data/images/b1ca7390-IMG_6916.jpeg', 'data/images/dc05dee6-IMG_6926.jpeg', 'data/images/e7d02260-IMG_6932.jpeg', 'data/images/ef7e6683-IMG_6915.jpeg', 'data/images/f54cb875-IMG_6928.jpeg', 'data/images/f65a8941-IMG_6925.jpeg']\n",
      "25 classes: [['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment'], ['Bud', 'Stem', 'Leaf', 'Leaf-Attachment']]\n",
      "25 bbox: [[[0.0, 164.0, 51.0, 246.0], [327.0, 31.0, 380.0, 91.0], [343.0, 5.0, 374.0, 30.0], [303.0, 266.0, 341.0, 308.0], [231.0, 294.0, 274.0, 322.0], [114.0, 271.0, 216.0, 343.0], [342.0, 96.0, 364.0, 263.0], [287.0, 286.0, 308.0, 297.0]], [[46.0, 183.0, 128.0, 242.0], [120.0, 50.0, 169.0, 115.0], [237.0, 286.0, 273.0, 318.0], [312.0, 274.0, 344.0, 293.0], [364.0, 232.0, 459.0, 294.0], [162.0, 129.0, 228.0, 263.0], [276.0, 285.0, 292.0, 295.0]], [[59.0, 131.0, 160.0, 246.0], [161.0, 254.0, 272.0, 424.0], [15.0, 34.0, 124.0, 131.0], [157.0, 213.0, 195.0, 250.0]], [[231.0, 174.0, 320.0, 316.0], [253.0, 514.0, 316.0, 606.0], [324.0, 591.0, 374.0, 637.0], [66.0, 266.0, 113.0, 318.0], [83.0, 224.0, 113.0, 270.0], [147.0, 464.0, 162.0, 634.0], [109.0, 300.0, 155.0, 464.0], [247.0, 320.0, 305.0, 507.0], [202.0, 87.0, 236.0, 181.0], [416.0, 86.0, 452.0, 235.0], [138.0, 42.0, 180.0, 157.0], [161.0, 132.0, 209.0, 154.0]], [[71.0, 38.0, 159.0, 161.0], [190.0, 128.0, 275.0, 213.0], [384.0, 322.0, 496.0, 407.0], [49.0, 200.0, 119.0, 286.0], [140.0, 213.0, 191.0, 285.0], [193.0, 243.0, 262.0, 299.0], [286.0, 334.0, 354.0, 403.0], [0.0, 0.0, 50.0, 189.0], [2.0, 8.0, 87.0, 57.0]], [[48.0, 249.0, 116.0, 332.0], [49.0, 48.0, 84.0, 88.0], [36.0, 5.0, 69.0, 46.0], [171.0, 14.0, 259.0, 106.0], [243.0, 100.0, 319.0, 143.0], [285.0, 189.0, 335.0, 240.0], [200.0, 350.0, 260.0, 449.0], [434.0, 223.0, 489.0, 295.0], [64.0, 79.0, 80.0, 247.0], [230.0, 128.0, 287.0, 195.0], [319.0, 244.0, 367.0, 300.0], [317.0, 120.0, 351.0, 167.0], [352.0, 155.0, 385.0, 183.0], [372.0, 173.0, 416.0, 212.0], [473.0, 352.0, 568.0, 449.0], [82.0, 1.0, 176.0, 68.0], [155.0, 52.0, 207.0, 116.0], [287.0, 147.0, 324.0, 190.0], [346.0, 212.0, 371.0, 253.0], [389.0, 277.0, 416.0, 340.0], [394.0, 343.0, 427.0, 394.0], [264.0, 371.0, 320.0, 418.0], [255.0, 236.0, 313.0, 272.0]], [[40.0, 215.0, 164.0, 309.0], [202.0, 121.0, 341.0, 275.0], [0.0, 304.0, 76.0, 341.0], [157.0, 232.0, 196.0, 275.0]], [[16.0, 325.0, 63.0, 406.0], [278.0, 240.0, 368.0, 354.0], [217.0, 144.0, 266.0, 189.0], [141.0, 129.0, 171.0, 180.0], [98.0, 66.0, 142.0, 126.0], [325.0, 361.0, 371.0, 416.0], [357.0, 202.0, 389.0, 266.0], [149.0, 98.0, 184.0, 131.0], [262.0, 204.0, 306.0, 237.0]], [[92.0, 179.0, 178.0, 267.0], [267.0, 260.0, 330.0, 319.0], [291.0, 287.0, 393.0, 381.0], [184.0, 206.0, 267.0, 264.0]], [[192.0, 100.0, 270.0, 182.0], [294.0, 62.0, 388.0, 136.0], [421.0, 211.0, 461.0, 291.0], [319.0, 208.0, 415.0, 428.0]], [[96.0, 50.0, 168.0, 128.0], [188.0, 139.0, 264.0, 209.0], [338.0, 263.0, 467.0, 415.0], [240.0, 205.0, 315.0, 278.0], [152.0, 122.0, 192.0, 165.0]], [[64.0, 137.0, 137.0, 226.0], [179.0, 281.0, 287.0, 401.0], [75.0, 226.0, 174.0, 312.0], [18.0, 157.0, 60.0, 184.0], [0.0, 166.0, 59.0, 218.0]], [[114.0, 42.0, 219.0, 127.0], [203.0, 144.0, 276.0, 223.0], [272.0, 196.0, 350.0, 265.0], [353.0, 308.0, 481.0, 419.0], [319.0, 235.0, 402.0, 315.0], [85.0, 18.0, 143.0, 87.0], [138.0, 58.0, 210.0, 150.0]], [[400.0, 118.0, 505.0, 189.0], [310.0, 197.0, 404.0, 270.0], [235.0, 235.0, 328.0, 314.0], [520.0, 98.0, 583.0, 160.0], [500.0, 140.0, 521.0, 153.0], [495.0, 149.0, 528.0, 168.0], [404.0, 176.0, 492.0, 240.0], [322.0, 252.0, 362.0, 284.0], [223.0, 314.0, 233.0, 320.0], [279.0, 302.0, 291.0, 330.0]], [[43.0, 356.0, 92.0, 443.0], [1.0, 234.0, 58.0, 271.0], [49.0, 46.0, 134.0, 156.0], [234.0, 163.0, 281.0, 212.0], [144.0, 117.0, 188.0, 191.0], [257.0, 277.0, 329.0, 364.0], [375.0, 235.0, 424.0, 283.0], [68.0, 0.0, 139.0, 60.0], [13.0, 1.0, 47.0, 54.0], [298.0, 206.0, 339.0, 241.0], [342.0, 238.0, 372.0, 272.0], [335.0, 295.0, 352.0, 321.0], [217.0, 243.0, 251.0, 290.0]], [[9.0, 320.0, 74.0, 410.0], [236.0, 176.0, 307.0, 264.0], [137.0, 56.0, 163.0, 95.0], [101.0, 24.0, 129.0, 63.0], [392.0, 208.0, 416.0, 249.0], [379.0, 244.0, 401.0, 299.0], [357.0, 289.0, 383.0, 336.0], [340.0, 334.0, 362.0, 368.0], [323.0, 358.0, 347.0, 394.0], [166.0, 84.0, 235.0, 191.0], [131.0, 33.0, 150.0, 55.0]], [[111.0, 59.0, 182.0, 147.0], [213.0, 135.0, 273.0, 219.0], [260.0, 198.0, 344.0, 278.0], [338.0, 293.0, 460.0, 425.0], [313.0, 229.0, 395.0, 312.0], [74.0, 0.0, 158.0, 58.0], [116.0, 38.0, 136.0, 60.0], [152.0, 54.0, 240.0, 132.0], [330.0, 211.0, 370.0, 226.0]], [[6.0, 152.0, 100.0, 234.0], [178.0, 249.0, 237.0, 301.0], [204.0, 275.0, 297.0, 363.0], [103.0, 175.0, 190.0, 245.0]], [[309.0, 488.0, 399.0, 931.0], [240.0, 292.0, 438.0, 496.0], [155.0, 442.0, 265.0, 527.0], [628.0, 688.0, 671.0, 942.0]], [[48.0, 318.0, 126.0, 418.0], [160.0, 229.0, 231.0, 322.0], [207.0, 174.0, 273.0, 244.0], [268.0, 9.0, 378.0, 147.0], [243.0, 116.0, 334.0, 212.0], [38.0, 425.0, 76.0, 476.0], [64.0, 416.0, 86.0, 446.0], [127.0, 323.0, 197.0, 386.0], [111.0, 382.0, 136.0, 413.0], [89.0, 409.0, 123.0, 426.0], [70.0, 425.0, 93.0, 444.0], [276.0, 208.0, 326.0, 218.0], [230.0, 237.0, 260.0, 276.0]], [[186.0, 106.0, 271.0, 193.0], [423.0, 216.0, 466.0, 301.0], [344.0, 199.0, 417.0, 398.0]], [[200.0, 200.0, 338.0, 343.0], [40.0, 101.0, 169.0, 209.0], [0.0, 46.0, 68.0, 104.0], [152.0, 169.0, 225.0, 220.0]], [[95.0, 336.0, 193.0, 407.0], [187.0, 241.0, 290.0, 316.0], [385.0, 95.0, 499.0, 207.0], [268.0, 207.0, 338.0, 271.0], [305.0, 141.0, 399.0, 235.0], [13.0, 354.0, 94.0, 433.0], [111.0, 279.0, 186.0, 336.0], [245.0, 226.0, 268.0, 249.0], [292.0, 187.0, 334.0, 204.0], [349.0, 183.0, 394.0, 196.0]], [[221.0, 49.0, 312.0, 164.0], [272.0, 182.0, 347.0, 284.0], [356.0, 314.0, 441.0, 474.0], [119.0, 146.0, 175.0, 204.0], [5.0, 0.0, 93.0, 95.0], [13.0, 0.0, 139.0, 21.0]], [[219.0, 151.0, 296.0, 253.0], [339.0, 196.0, 414.0, 392.0], [425.0, 217.0, 471.0, 291.0]]]\n",
      "class counts: {'Bud': 103, 'Stem': 102, 'Leaf': 80, 'Leaf-Attachment': 25}\n",
      "file counts: {'Bud': 27, 'Stem': 26, 'Leaf': 17, 'Leaf-Attachment': 12}\n"
     ]
    }
   ],
   "source": [
    "# export PascalVOC images + annotations from Label Studio\n",
    "\n",
    "class_counts = dict.fromkeys(class_ids, 0)\n",
    "file_counts = dict.fromkeys(class_ids, 0)\n",
    "\n",
    "def parse_annotation(xml_file):\n",
    "    local_class_counts = dict.fromkeys(class_ids, 0)\n",
    "#    print(local_class_counts)\n",
    "  \n",
    "    classes_seen = set()\n",
    "    \n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    image_name = root.find(\"filename\").text\n",
    "    image_path = os.path.join(path_images, image_name)\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f'image at {image_path} not found')\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "\n",
    "    has_marked_class_in_file = False\n",
    "    \n",
    "    for obj in root.iter(\"object\"):\n",
    "        cls = obj.find(\"name\").text\n",
    "        classes_seen.add(cls)\n",
    "\n",
    "#        print(f'here, cls is \"{cls}\", lcc is {local_class_counts}')\n",
    "        \n",
    "        local_class_counts[cls] += 1\n",
    "        class_counts[cls] += 1\n",
    "        \n",
    "        if not cls in class_mapping_values:\n",
    "            continue\n",
    "        \n",
    "        classes.append(cls)\n",
    "\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        xmin = float(bbox.find(\"xmin\").text)\n",
    "        ymin = float(bbox.find(\"ymin\").text)\n",
    "        xmax = float(bbox.find(\"xmax\").text)\n",
    "        ymax = float(bbox.find(\"ymax\").text)\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    local_class_ids = [\n",
    "        list(class_mapping.keys())[list(class_mapping_values).index(cls)]\n",
    "        for cls in classes\n",
    "    ]\n",
    "\n",
    "    for cls in classes_seen:\n",
    "        file_counts[cls] += 1        \n",
    "    \n",
    "    print(f'{image_name}, {local_class_counts}')\n",
    "    \n",
    "    return image_path, boxes, local_class_ids\n",
    "\n",
    "\n",
    "image_paths = []\n",
    "bbox = []\n",
    "classes = []\n",
    "for xml_file in tqdm(xml_files):\n",
    "    image_path, boxes, local_class_ids = parse_annotation(xml_file)\n",
    "    if len(set(local_class_ids)) == len(class_mapping_values):\n",
    "        image_paths.append(image_path)\n",
    "        bbox.append(boxes)\n",
    "        classes.append(class_ids)\n",
    "\n",
    "print(f'{len(image_paths)} image paths: {image_paths}')\n",
    "print(f'{len(classes)} classes: {classes}')\n",
    "print(f'{len(bbox)} bbox: {bbox}')\n",
    "print(f'class counts: {class_counts}')\n",
    "print(f'file counts: {file_counts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe1b1ee0-3eb8-4169-a5b2-d8a7db8fcfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), RaggedTensorSpec(TensorShape([None]), tf.string, 0, tf.int64), RaggedTensorSpec(TensorShape([None, None]), tf.float32, 1, tf.int64))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 19:41:06.635227: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2024-05-19 19:41:06.635252: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2024-05-19 19:41:06.635301: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2024-05-19 19:41:06.635366: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-19 19:41:06.635390: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "if USE_RAGGED_TENSORS:\n",
    "    bbox = tf.ragged.constant(bbox)\n",
    "    classes = tf.ragged.constant(classes)\n",
    "    image_paths = tf.ragged.constant(image_paths)\n",
    "else:\n",
    "    bbox = tf.constant(bbox)\n",
    "    classes = tf.constant(classes)\n",
    "    image_paths = tf.constant(image_paths)\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb1a68a-76d6-46bd-bc8c-60b852b45530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_val: 5\n",
      "val_data: <_TakeDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), RaggedTensorSpec(TensorShape([None]), tf.string, 0, tf.int64), RaggedTensorSpec(TensorShape([None, None]), tf.float32, 1, tf.int64))>, cardinality: 5\n",
      "train_data: <_SkipDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), RaggedTensorSpec(TensorShape([None]), tf.string, 0, tf.int64), RaggedTensorSpec(TensorShape([None, None]), tf.float32, 1, tf.int64))>, cardinality: 20\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of validation samples\n",
    "num_val = int(len(image_paths) * SPLIT_RATIO)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "val_data = data.take(num_val)\n",
    "train_data = data.skip(num_val)\n",
    "\n",
    "print(f'num_val: {num_val}')\n",
    "print(f'val_data: {val_data}, cardinality: {val_data.cardinality()}')\n",
    "print(f'train_data: {train_data}, cardinality: {train_data.cardinality()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56fa6c5e-614d-4017-8fa5-0fbbdf360e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    return image\n",
    "\n",
    "def load_dataset(image_path, classes, bbox):\n",
    "    print(f'load_dataset({image_path}, {classes}, {bbox})')\n",
    "    \n",
    "    # Read Image\n",
    "    image = load_image(image_path)\n",
    "\n",
    "    bounding_boxes = {\n",
    "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
    "        \"boxes\": bbox,\n",
    "    }\n",
    "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b34f05d9-1ca7-4afa-8344-1fd1a87cf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizes images to 640x640 while maintaining aspect ratio.\n",
    "# The bounding boxes associated with the image are specified in the xyxy format.\n",
    "# If necessary, the resized image will be padded with zeros to maintain the original aspect ratio.\n",
    "\n",
    "augmenter = keras.Sequential(\n",
    "    layers=[\n",
    "#        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n",
    "#        keras_cv.layers.RandomShear(\n",
    "#            x_factor=0.2, y_factor=0.2, bounding_box_format=\"xyxy\"\n",
    "#        ),\n",
    "        keras_cv.layers.JitteredResize(\n",
    "            target_size=(640, 640),\n",
    "            scale_factor=(SCALE_MIN, SCALE_MAX),\n",
    "            bounding_box_format=\"xyxy\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0845d6-764f-48aa-8680-a4f439b4d918",
   "metadata": {},
   "source": [
    "# Creating Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c65ed0-6a5f-4b77-afa2-0e611711d576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_dataset(Tensor(\"args_0:0\", shape=(), dtype=string), Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(None,), dtype=string), tf.RaggedTensor(values=Tensor(\"RaggedFromVariant_1/RaggedTensorFromVariant:1\", shape=(None,), dtype=float32), row_splits=Tensor(\"RaggedFromVariant_1/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64)))\n"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE was 4, and we may want to use this for larger datasets, but for now shuffle the full set\n",
    "# using the dataset's cardinality\n",
    "\n",
    "train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(BATCH_SIZE * 4)\n",
    "if USE_RAGGED_TENSORS:\n",
    "    train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663d350-64f2-41ad-8211-b9bc4e977f7b",
   "metadata": {},
   "source": [
    "# Creating Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9660d4bb-477d-4e4d-a6b6-b55e8b5adcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_dataset(Tensor(\"args_0:0\", shape=(), dtype=string), Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(None,), dtype=string), tf.RaggedTensor(values=Tensor(\"RaggedFromVariant_1/RaggedTensorFromVariant:1\", shape=(None,), dtype=float32), row_splits=Tensor(\"RaggedFromVariant_1/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64)))\n"
     ]
    }
   ],
   "source": [
    "resizing = keras_cv.layers.JitteredResize(\n",
    "    target_size=(640, 640),\n",
    "    scale_factor=(SCALE_MIN, SCALE_MAX),\n",
    "    bounding_box_format=\"xyxy\",\n",
    ")\n",
    "\n",
    "val_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.shuffle(val_ds.cardinality())\n",
    "if USE_RAGGED_TENSORS:\n",
    " val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba8bed6-be50-4db8-a422-86df30677526",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bd70ec6-f91e-49bf-8caa-ee0099b35e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds: <_ParallelMapDataset element_spec={'images': TensorSpec(shape=(4, 640, 640, 3), dtype=tf.float32, name=None), 'bounding_boxes': {'classes': RaggedTensorSpec(TensorShape([4, None]), tf.float32, 1, tf.int64), 'boxes': RaggedTensorSpec(TensorShape([4, None, None]), tf.float32, 1, tf.int64)}}>\n",
      "val_ds: <_ParallelMapDataset element_spec={'images': TensorSpec(shape=(4, 640, 640, 3), dtype=tf.float32, name=None), 'bounding_boxes': {'classes': RaggedTensorSpec(TensorShape([4, None]), tf.float32, 1, tf.int64), 'boxes': RaggedTensorSpec(TensorShape([4, None, None]), tf.float32, 1, tf.int64)}}>\n",
      "<_ParallelMapDataset element_spec={'images': TensorSpec(shape=(4, 640, 640, 3), dtype=tf.float32, name=None), 'bounding_boxes': {'classes': RaggedTensorSpec(TensorShape([4, None]), tf.float32, 1, tf.int64), 'boxes': RaggedTensorSpec(TensorShape([4, None, None]), tf.float32, 1, tf.int64)}}>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 19:41:09.203612: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-05-19 19:41:09.284228: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.286453: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.288583: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.290735: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.292981: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.295260: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.297595: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.300064: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.302343: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.304578: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.306818: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.309241: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.311507: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.313720: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.316010: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.318233: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.320466: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.322702: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.324894: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.327128: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.329380: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.331616: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.333830: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.336023: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.338223: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.340408: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.342639: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.344870: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.347077: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.349307: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.351558: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.353765: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.356269: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.358671: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.360895: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.363078: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.365230: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.368085: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.370290: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.372486: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.374872: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.377454: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.379827: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.382240: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.384523: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.386764: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.389025: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.391205: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.393434: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.395651: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.397855: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.400049: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.402252: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.404452: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.406896: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.409231: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.411542: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.414012: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.416363: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2024-05-19 19:41:09.418645: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cast string to float is not supported\n\t [[{{node Cast}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(bounding_boxes)\n\u001b[1;32m     12\u001b[0m     visualization\u001b[38;5;241m.\u001b[39mplot_bounding_box_gallery(\n\u001b[1;32m     13\u001b[0m         images,\n\u001b[1;32m     14\u001b[0m         value_range\u001b[38;5;241m=\u001b[39mvalue_range,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         class_mapping\u001b[38;5;241m=\u001b[39mclass_mapping,\n\u001b[1;32m     22\u001b[0m     )\n\u001b[0;32m---> 24\u001b[0m \u001b[43mvisualize_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounding_box_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxyxy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m visualize_dataset(\n\u001b[1;32m     28\u001b[0m     val_ds, bounding_box_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxyxy\u001b[39m\u001b[38;5;124m\"\u001b[39m, value_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     29\u001b[0m )\n",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m, in \u001b[0;36mvisualize_dataset\u001b[0;34m(inputs, value_range, rows, cols, bounding_box_format)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_dataset\u001b[39m(inputs, value_range, rows, cols, bounding_box_format):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inputs)\n\u001b[0;32m----> 6\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     images, bounding_boxes \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m], inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbounding_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images)\n",
      "File \u001b[0;32m~/code/osu/cs462/keras2-yolo8/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:809\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    808\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/code/osu/cs462/keras2-yolo8/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:772\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 772\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/code/osu/cs462/keras2-yolo8/venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3028\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3028\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3030\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/code/osu/cs462/keras2-yolo8/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5888\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5886\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5887\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5888\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cast string to float is not supported\n\t [[{{node Cast}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "print(f'train_ds: {train_ds}')\n",
    "print(f'val_ds: {val_ds}')\n",
    "\n",
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
    "    print(inputs)\n",
    "    inputs = next(iter(inputs.take(1)))\n",
    "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "\n",
    "    print(images)\n",
    "    print(bounding_boxes)\n",
    "    \n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.7,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "visualize_dataset(\n",
    "    train_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n",
    ")\n",
    "visualize_dataset(\n",
    "    val_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b80159-0829-4327-906b-2cba7a1a75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "\n",
    "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a505e5a-d14d-4f30-8211-2ccd9738a437",
   "metadata": {},
   "source": [
    "# Create The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097ef85-2154-4709-b05c-bf16d88377c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "    \"yolo_v8_s_backbone_coco\"  # We will use yolov8 small backbone with coco weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c915d4-c07d-4ba5-842f-7ef95e3e41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'len(class_mapping) is {len(class_mapping)}')\n",
    "\n",
    "yolo = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=len(class_mapping),\n",
    "    bounding_box_format=\"xyxy\",\n",
    "    backbone=backbone,\n",
    "    fpn_depth=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776bc512-fb4a-4843-b13b-5fa100eb2810",
   "metadata": {},
   "source": [
    "# Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de54a8a-9106-4e6e-a216-610903bbbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using optimizers.legacy.Adam instead of optimizers.Adam because of this message:\n",
    "#\n",
    "# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs,\n",
    "# please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    global_clipnorm=GLOBAL_CLIPNORM,\n",
    ")\n",
    "\n",
    "yolo.compile(\n",
    "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6d154-01c8-4ceb-9932-1cd963412696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, data, save_path):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n",
    "            bounding_box_format=\"xyxy\",\n",
    "            evaluate_freq=1e9,\n",
    "        )\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.best_map = -1.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.metrics.reset_state()\n",
    "        for batch in self.data:\n",
    "            images, y_true = batch[0], batch[1]\n",
    "            y_pred = self.model.predict(images, verbose=0)\n",
    "            self.metrics.update_state(y_true, y_pred)\n",
    "\n",
    "        metrics = self.metrics.result(force=True)\n",
    "        logs.update(metrics)\n",
    "\n",
    "        current_map = metrics[\"MaP\"]\n",
    "        if current_map > self.best_map:\n",
    "            self.best_map = current_map\n",
    "            self.model.save(self.save_path, save_format=\"tf\")  # Save the model when mAP improves\n",
    "\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b672f6ee-7ca1-4e00-a8c7-4058550de3b9",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068b44c-7452-40a6-8a35-08fd2fe157a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[EvaluateCOCOMetricsCallback(val_ds, \"model.tf\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77b8d2-5df0-472b-9395-b9e1f69c3a9b",
   "metadata": {},
   "source": [
    "# Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808c102-0754-47d9-80c7-647d4ef502a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(model, dataset, bounding_box_format):\n",
    "    images, y_true = next(iter(dataset.take(1)))\n",
    "    y_pred = model.predict(images)\n",
    "    y_pred = bounding_box.to_ragged(y_pred)\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        scale=4,\n",
    "        rows=1,\n",
    "        cols=2,\n",
    "        show=True,\n",
    "        font_scale=0.7,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "visualize_detections(yolo, dataset=val_ds, bounding_box_format=\"xyxy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698fbe24-097c-4348-b690-147a1fdc7fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras2yolov8",
   "language": "python",
   "name": "keras2yolov8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
